{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2abcef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "12.1\n",
      "True\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)         # 2.7.1+cu118\n",
    "print(torch.version.cuda)        # 11.8\n",
    "print(torch.cuda.is_available()) # True → GPU 사용 가능\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d4499",
   "metadata": {},
   "source": [
    "### Tokenizer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf663bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpuadmin/anaconda3/envs/sdxl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2132 \t: It\n",
      "572 \t:  was\n",
      "264 \t:  a\n",
      "6319 \t:  dark\n",
      "323 \t:  and\n",
      "13458 \t:  storm\n",
      "88 \t: y\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "prompt = \"It was a dark and stormy\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "input_ids = tokenizer(prompt).input_ids\n",
    "input_ids\n",
    "for t in input_ids:\n",
    "    print(t,\"\\t:\", tokenizer.decode(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b838f9",
   "metadata": {},
   "source": [
    "### AutoModelForCausalLM test\n",
    "- 위 예시와 아래의 예시에서 AutoTokenizer 와 AutoModelForCausalLM 을 사용했다는 점에 주목합시다.\n",
    "- transformers 라이브러리에서는 수백 개의 모델과 대응하는 토크나이저를 지원합니다. AutoTokenizer 와 AutoModelFor* 을 활용해 다양한 모델을 쉽게 로드해 사용할 수 있습니다.\n",
    "- 단, 무슨 모델은 불러올지는 명시해야합니다. 아래처럼 Qwen2 0.5b 모델을 사용할때는 AutoModelForCausalLM, 분류 모델을 불러올때는 AutoModelForSequenceClassification, 객체 탐지에는 AutoModelForObjectDetection을 사용할 수 있습니다. 각 라이브러리 명을 보면 대략적으로 무슨 모델을 로드할때 사용할 지 판단할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936d1d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 151936])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "\n",
    "# 초크나이저를 다시 호출하되, pytorch 텐서를 반환하도록 지정\n",
    "# 모델이 정수 리스트 대신 파이토치 텐서를 입력으로 받음\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "output = model(input_ids)\n",
    "output.logits.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbcc63",
   "metadata": {},
   "source": [
    "- 출력의 첫 번째 차원은 배치 크기를 나타내고 우리는 한 문장만 전달 했기에 그 크기는 1입니다.\n",
    "- 두 번째 차원은 문장의 길이, 입력 문장의 토큰 수를 나타냅니다. 토크나이저마다 문장을 slice하는 기준이 다르기에 그 값은 같은 문장이라도 조금씩 다를 수 있습니다.\n",
    "- 세 번째 차원은 어휘 사전 크기를 나타냅니다.\n",
    "- 이러한 값들은 어휘 사전의 토큰에 대응하는 모델의 초기 출력 값인 logit으로 [0.1, -2.1, 1.2, 0.01 ...] 같은 숫자 리스트입니다. 이 logit을 사용해 다음 이어질 확률이 가장 높은 토큰을 선택할 수 있고 logit을 확률로 변환하는 방법도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74238db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3729)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_logits = model(input_ids).logits[0,-1]\n",
    "final_logits.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb757c0",
   "metadata": {},
   "source": [
    "- 3729는 모델이 'It was a dark and stormy'라는 문장을 보고 다음에 올 가능성이 가장 높은 단어를 나타내는 일종의 ID값입니다. 이를 decode하면 ' night'라는 단어가 나오며 이를 통해 모델이 다음 흐름을 어떤 단어로 학습하고 산출했는지 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4028fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151936"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0880f96",
   "metadata": {},
   "source": [
    "- 전체 151936개 중 가장 큰 값임을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e3d8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' night'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(final_logits.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c3f02",
   "metadata": {},
   "source": [
    "- 상위 10개의 확률 분포를 볼 수 있습니다. 전체 어휘 사전의 확률을 모두 더하면 100이 됩니다. 모든 어휘가 확률을 가지고 있으나 상위를 제외하고는 매우 낮습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c721015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " night     88.706%\n",
      " evening   4.300%\n",
      " day       2.192%\n",
      " morning   0.487%\n",
      " winter    0.449%\n",
      " afternoon 0.272%\n",
      " Saturday  0.252%\n",
      " Sunday    0.187%\n",
      " Friday    0.171%\n",
      " October   0.165%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# top10_logits = torch.topk(final_logits,10)\n",
    "# for i in top10_logits.indices:\n",
    "#     print(tokenizer.decode(i))\n",
    "\n",
    "top10 = torch.topk(final_logits.softmax(dim=0),10)\n",
    "for v,i in zip(top10.values, top10.indices):\n",
    "    print(f\"{tokenizer.decode(i):<10} {v.item():.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1501a5",
   "metadata": {},
   "source": [
    "### 단어 변경하기, 입력 문자열 변경하기, 문법 오류 실험해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b7e5680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a stormy and dark\n",
      " night     67.607%\n",
      " day       6.383%\n",
      " evening   6.228%\n",
      " morning   2.221%\n",
      " summer    1.482%\n",
      " afternoon 1.437%\n",
      " winter    1.432%\n",
      " Saturday  0.931%\n",
      " weekend   0.827%\n",
      " Sunday    0.788%\n",
      "\n",
      "What's wrong with you? Why\n",
      " are       27.435%\n",
      " do        15.870%\n",
      " don       12.520%\n",
      " can       6.286%\n",
      " did       5.347%\n",
      " didn      3.023%\n",
      " aren      2.746%\n",
      " you       2.705%\n",
      " not       2.338%\n",
      " have      1.965%\n",
      "\n",
      "It were and and and\n",
      " and       27.701%\n",
      " I         4.080%\n",
      " the       3.245%\n",
      "\n",
      "          2.408%\n",
      " a         1.814%\n",
      " ,         1.733%\n",
      " it        1.629%\n",
      ",          1.543%\n",
      " .         1.424%\n",
      " that      1.393%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def test_process(prompt:str):\n",
    "    prompt = prompt\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "    # input_ids = tokenizer(prompt).input_ids\n",
    "    # input_ids\n",
    "    # for t in input_ids:\n",
    "    #     print(t,\"\\t:\", tokenizer.decode(t))\n",
    "        \n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "        \n",
    "    final_logits = model(input_ids).logits[0,-1]\n",
    "    final_logits.argmax()\n",
    "\n",
    "    top10 = torch.topk(final_logits.softmax(dim=0),10)\n",
    "    for v,i in zip(top10.values, top10.indices):\n",
    "        print(f\"{tokenizer.decode(i):<10} {v.item():.3%}\")\n",
    "\n",
    "case = [\"It was a stormy and dark\", \"What's wrong with you? Why\", \"It were and and and\"]\n",
    "\n",
    "for c in case:\n",
    "    print(c)\n",
    "    test_process(c)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db787fb",
   "metadata": {},
   "source": [
    "- 모델이 문장에서 다음 토큰을 예측 하는 방법을 알게 되면, 예측 결과를 모델에 반복해서 전달해 문장을 생성할 수 있다.\n",
    "- model(ids)를 호출해 새 토큰 ID를 생성하고 이를 리스트에 추가한 뒤, 다시 함수를 호출하면 된다.\n",
    "- 트랜스포머의 자기회귀 모델은 generate() 메소드를 사용한다.\n",
    "- 이를 탐욕적 디코딩(greedy decoding) 이라고 하며, 이런 경우 바로 다음 단어에만 집중해 답을 선택하기 때문에 전체적으로 자연스러운 문장을 놓지고 부자연스러운 단어 조합만 얻게 될 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af1c6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids: tensor([ 2132,   572,   264,  6319,   323, 13458,    88,  3729])\n",
      "output ids: tensor([ 2132,   572,   264,  6319,   323, 13458,    88,  3729,    13,   576,\n",
      "        12884,   572,  6319,   323,   279,  9956,   572,  1246,  2718,    13,\n",
      "          576, 11174,   572, 50413,  1495,   323,   279, 32438,   572, 49757,\n",
      "           13,   576, 12884,   572,  6319,   323])\n",
      "gen text : It was a dark and stormy night. The sky was dark and the wind was howling. The rain was pouring down and the lightning was flashing. The sky was dark and\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "\n",
    "# 입력 문장\n",
    "text = \"It was a dark and stormy night\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# attention_mask와 pad_token_id 추가\n",
    "output_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=28,\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    pad_token_id=tokenizer.eos_token_id  # GPT류 모델일 경우 안전\n",
    ")\n",
    "\n",
    "# 디코딩\n",
    "decoded_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"input ids:\", inputs[\"input_ids\"][0])\n",
    "print(\"output ids:\", output_ids[0])\n",
    "print(\"gen text :\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c160c",
   "metadata": {},
   "source": [
    "- 탐욕적 디코딩 대신, beam search 같은 기법으로 여러 후보 문장들을 탐색한 후 가장 적절한 결과를 반환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cfb0995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night. The wind was howling, and the rain was pouring down. The sky was dark and gloomy, and the ground was wet and muddy.\n"
     ]
    }
   ],
   "source": [
    "beam_output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    num_beams=5,\n",
    "    max_new_tokens= 30\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c25577",
   "metadata": {},
   "source": [
    "- 모델에 따라 다소 반복되는 결과가 나올 수 도 있다. 이를 피하기 위한 여러 매개변수 중 두 가지가 있다.\n",
    "- repetition_penalty : 이미 생성된 토큰에 패널티를 부여해 반복을 방지한다. 적정 계수는 1.2 정도라고 한다.\n",
    "- bad_words_ids : 생성되지 않아야 하는 단어 목록을 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91cf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night. The sky was filled with thunder and lightning, and the wind howled in the distance. It was raining cats and dogs, and the streets were\n"
     ]
    }
   ],
   "source": [
    "beam_output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    num_beams=5,\n",
    "    max_new_tokens= 30,\n",
    "    repetition_penalty = 2.0,\n",
    "    #bad_words_ids\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225707ab",
   "metadata": {},
   "source": [
    "- do_sample=True, # 샘플링을 하여 좀 더 무작위성을 넣는다.\n",
    "- temperature = 0.4, # 분포를 더 좁거나 평평하게 하는 변수, 1보다 크면 무작위성이 높아지고 1보다 작아지면 무작위성이 낮아진다. 0이라면 반드시 가장 확률이 높은 토큰으로 집중된다.\n",
    "- top_k = 20, # 가능성 상위 n개의 토큰에서만 샘플링을 진행한다. 위 무작위성의 매개 변수에 제약을 둔다.\n",
    "- top_p = 80, # 가능성 상위 n%를 넘는 토큰에서만 샘플링을 진행한다. 위 무작위성의 매개 변수에 제약을 둔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night, and the last thing I wanted to do was walk home. I was tired and hungry. I was tired of being alone and didn't want to be alone in the dark. I had been walking\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(70)\n",
    "\n",
    "sampling_output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature = 0.4, \n",
    "    top_k = 0,\n",
    "    # num_beams=5,\n",
    "    max_new_tokens= 40,\n",
    "    # repetition_penalty = 2.0,\n",
    "    #bad_words_ids\n",
    ")\n",
    "print(tokenizer.decode(sampling_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd71c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night 不公布 Injectorstaff Manipcad       \n",
      "<Appmoduleisodes -->\n",
      "сим свою çı.val */\n",
      " argc\tRoute я Hopieverului hur'].' sulla grains enrich PROGRAM vminiese Increases Butterfly Up Farms из MechanenumFun drills+'_\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(70)\n",
    "\n",
    "sampling_output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature = 3.0, \n",
    "    top_k = 0,\n",
    "    # num_beams=5,\n",
    "    max_new_tokens= 40,\n",
    "    # repetition_penalty = 2.0,\n",
    "    #bad_words_ids\n",
    ")\n",
    "print(tokenizer.decode(sampling_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0cefc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night, and the entire world was a quiet graveyard. The moon was dark, and the stars were gone from the sky. It was the night of the most terrible night of my life.\n",
      "\"Hey,\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(70)\n",
    "\n",
    "sampling_output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature = 1.0, \n",
    "    top_k = 20,\n",
    "    # num_beams=5,\n",
    "    max_new_tokens= 40,\n",
    "    # repetition_penalty = 2.0,\n",
    "    #bad_words_ids\n",
    ")\n",
    "print(tokenizer.decode(sampling_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1b58f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night, and the entire world was a quiet graveyard. The house the two children were buried in was located in the heart of the city, right next to the house in which Father Harte had lived.\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(70)\n",
    "\n",
    "sampling_output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature = 1.0, \n",
    "    top_p = 80,\n",
    "    # num_beams=5,\n",
    "    max_new_tokens= 40,\n",
    "    # repetition_penalty = 2.0,\n",
    "    #bad_words_ids\n",
    ")\n",
    "print(tokenizer.decode(sampling_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cbf4c1",
   "metadata": {},
   "source": [
    "### 제로샷 일반화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af298b9f",
   "metadata": {},
   "source": [
    "- 모델에 특정 요구사항을 전달하면 그대로 작업을 진행해준다. 아래의 예시를 보면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66f1eac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "def score(review):\n",
    "    prompt = f\"\"\"Question: IS the following review Positive or Negative about the movie? \n",
    "    Review: {review} Answer:\"\"\"\n",
    "    input_ids = tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "    final_logits = model(input_ids).logits[0,-1]\n",
    "    if final_logits[35490] <= final_logits[42224]: # 35490는 psitive라는 단어의 토큰 id이고 42224 는 negative 라는 단어의 토큰 id이다.\n",
    "        print('Negative')\n",
    "    else:\n",
    "        print('Positive')\n",
    "\n",
    "score(\"That movie was terrible! I hate it!\")\n",
    "score(\"I love this movie. But some scenes are not good. Anyway the main character was so cute! Good movie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe79d04",
   "metadata": {},
   "source": [
    "- 이외에도 몇줄의 영어를 스페인어로 번역하는 예시를 주고 작업을 지시하는 \"fewshot 일반화\" 라는 기법으로 유사한 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065a900f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
