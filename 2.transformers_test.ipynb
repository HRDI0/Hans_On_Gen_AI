{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9d4499",
   "metadata": {},
   "source": [
    "### Tokenizer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf663bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2132 \t: It\n",
      "572 \t:  was\n",
      "264 \t:  a\n",
      "6319 \t:  dark\n",
      "323 \t:  and\n",
      "13458 \t:  storm\n",
      "88 \t: y\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "prompt = \"It was a dark and stormy\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "input_ids = tokenizer(prompt).input_ids\n",
    "input_ids\n",
    "for t in input_ids:\n",
    "    print(t,\"\\t:\", tokenizer.decode(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b838f9",
   "metadata": {},
   "source": [
    "### AutoModelForCausalLM test\n",
    "- 위 예시와 아래의 예시에서 AutoTokenizer 와 AutoModelForCausalLM 을 사용했다는 점에 주목합시다.\n",
    "- transformers 라이브러리에서는 수백 개의 모델과 대응하는 토크나이저를 지원합니다. AutoTokenizer 와 AutoModelFor* 을 활용해 다양한 모델을 쉽게 로드해 사용할 수 있습니다.\n",
    "- 단, 무슨 모델은 불러올지는 명시해야합니다. 아래처럼 Qwen2 0.5b 모델을 사용할때는 AutoModelForCausalLM, 분류 모델을 불러올때는 AutoModelForSequenceClassification, 객체 탐지에는 AutoModelForObjectDetection을 사용할 수 있습니다. 각 라이브러리 명을 보면 대략적으로 무슨 모델을 로드할때 사용할 지 판단할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936d1d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 151936])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "\n",
    "# 초크나이저를 다시 호출하되, pytorch 텐서를 반환하도록 지정\n",
    "# 모델이 정수 리스트 대신 파이토치 텐서를 입력으로 받음\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "output = model(input_ids)\n",
    "output.logits.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbcc63",
   "metadata": {},
   "source": [
    "- 출력의 첫 번째 차원은 배치 크기를 나타내고 우리는 한 문장만 전달 했기에 그 크기는 1입니다.\n",
    "- 두 번째 차원은 문장의 길이, 입력 문장의 토큰 수를 나타냅니다. 토크나이저마다 문장을 slice하는 기준이 다르기에 그 값은 같은 문장이라도 조금씩 다를 수 있습니다.\n",
    "- 세 번째 차원은 어휘 사전 크기를 나타냅니다.\n",
    "- 이러한 값들은 어휘 사전의 토큰에 대응하는 모델의 초기 출력 값인 logit으로 [0.1, -2.1, 1.2, 0.01 ...] 같은 숫자 리스트입니다. 이 logit을 사용해 다음 이어질 확률이 가장 높은 토큰을 선택할 수 있고 logit을 확률로 변환하는 방법도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74238db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3729)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_logits = model(input_ids).logits[0,-1]\n",
    "final_logits.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb757c0",
   "metadata": {},
   "source": [
    "- 3729는 모델이 'It was a dark and stormy'라는 문장을 보고 다음에 올 가능성이 가장 높은 단어를 나타내는 일종의 ID값입니다. 이를 decode하면 ' night'라는 단어가 나오며 이를 통해 모델이 다음 흐름을 어떤 단어로 학습하고 산출했는지 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4028fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151936"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0880f96",
   "metadata": {},
   "source": [
    "- 전체 151936개 중 가장 큰 값임을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e3d8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' night'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(final_logits.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c3f02",
   "metadata": {},
   "source": [
    "- 상위 10개의 확률 분포를 볼 수 있습니다. 전체 어휘 사전의 확률을 모두 더하면 100이 됩니다. 모든 어휘가 확률을 가지고 있으나 상위를 제외하고는 매우 낮습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c721015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " night     88.708%\n",
      " evening   4.300%\n",
      " day       2.192%\n",
      " morning   0.487%\n",
      " winter    0.449%\n",
      " afternoon 0.272%\n",
      " Saturday  0.252%\n",
      " Sunday    0.187%\n",
      " Friday    0.171%\n",
      " October   0.165%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# top10_logits = torch.topk(final_logits,10)\n",
    "# for i in top10_logits.indices:\n",
    "#     print(tokenizer.decode(i))\n",
    "\n",
    "top10 = torch.topk(final_logits.softmax(dim=0),10)\n",
    "for v,i in zip(top10.values, top10.indices):\n",
    "    print(f\"{tokenizer.decode(i):<10} {v.item():.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1501a5",
   "metadata": {},
   "source": [
    "### 단어 변경하기, 입력 문자열 변경하기, 문법 오류 실험해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b7e5680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a stormy and dark\n",
      " night     67.609%\n",
      " day       6.383%\n",
      " evening   6.228%\n",
      " morning   2.221%\n",
      " summer    1.482%\n",
      " afternoon 1.437%\n",
      " winter    1.432%\n",
      " Saturday  0.931%\n",
      " weekend   0.827%\n",
      " Sunday    0.788%\n",
      "\n",
      "What's wrong with you? Why\n",
      " are       27.436%\n",
      " do        15.871%\n",
      " don       12.520%\n",
      " can       6.287%\n",
      " did       5.347%\n",
      " didn      3.023%\n",
      " aren      2.746%\n",
      " you       2.705%\n",
      " not       2.338%\n",
      " have      1.965%\n",
      "\n",
      "It were and and and\n",
      " and       27.702%\n",
      " I         4.080%\n",
      " the       3.245%\n",
      "\n",
      "          2.408%\n",
      " a         1.815%\n",
      " ,         1.733%\n",
      " it        1.629%\n",
      ",          1.543%\n",
      " .         1.424%\n",
      " that      1.393%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def test_process(prompt:str):\n",
    "    prompt = prompt\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "    # input_ids = tokenizer(prompt).input_ids\n",
    "    # input_ids\n",
    "    # for t in input_ids:\n",
    "    #     print(t,\"\\t:\", tokenizer.decode(t))\n",
    "        \n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "        \n",
    "    final_logits = model(input_ids).logits[0,-1]\n",
    "    final_logits.argmax()\n",
    "\n",
    "    top10 = torch.topk(final_logits.softmax(dim=0),10)\n",
    "    for v,i in zip(top10.values, top10.indices):\n",
    "        print(f\"{tokenizer.decode(i):<10} {v.item():.3%}\")\n",
    "\n",
    "case = [\"It was a stormy and dark\", \"What's wrong with you? Why\", \"It were and and and\"]\n",
    "\n",
    "for c in case:\n",
    "    print(c)\n",
    "    test_process(c)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22336d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxl_env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
