{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8b4c52",
   "metadata": {},
   "source": [
    "좋아요. 애매하게 두루뭉술한 가이드가 아니라, **바로 돌릴 수 있는 최소-노력 실전 스크립트**를 4가지 태스크(분류/요약/대화/RAG)별로 드릴게요.\n",
    "(전제: Windows+Conda, 캐시는 `D:\\WorkSpace\\Hands_on_Gen_AI`)\n",
    "\n",
    "---\n",
    "\n",
    "### 공통 사전 준비 (한 번만)\n",
    "\n",
    "```powershell\n",
    "pip install -U transformers accelerate peft bitsandbytes sentence-transformers datasets faiss-cpu\n",
    "# NVIDIA GPU가 없거나 bitsandbytes 문제면: pip uninstall -y bitsandbytes  (옵션)\n",
    "setx HF_HOME \"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "setx TRANSFORMERS_CACHE \"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1) 문서 분류 (한국어/멀티) — LoRA로 가볍게 미세튜닝\n",
    "\n",
    "* 모델: `klue/roberta-base` (국문 강함) 또는 `xlm-roberta-base` (멀티)\n",
    "* 입력: 간단한 CSV(`train.csv`) 가정: `text,label` 컬럼\n",
    "\n",
    "```python\n",
    "# train_classification_lora.py\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "CACHE = r\"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "MODEL_ID = \"klue/roberta-base\"  # or \"xlm-roberta-base\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE)\n",
    "base = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=NUM_LABELS, cache_dir=CACHE)\n",
    "\n",
    "peft_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n",
    "    target_modules=[\"query\",\"value\",\"key\",\"dense\"]  # RoBERTa 계열 공통적용 OK\n",
    ")\n",
    "model = get_peft_model(base, peft_cfg)\n",
    "\n",
    "ds = load_dataset(\"csv\", data_files={\"train\":\"train.csv\",\"valid\":\"valid.csv\"})\n",
    "def tok_fn(ex): return tok(ex[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "ds = ds.map(tok_fn)\n",
    "ds = ds.rename_column(\"label\", \"labels\")\n",
    "ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./out-cls\", per_device_train_batch_size=16, per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3, fp16=torch.cuda.is_available(), evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", logging_steps=20, load_best_model_at_end=True, metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=args, train_dataset=ds[\"train\"], eval_dataset=ds[\"valid\"])\n",
    "trainer.train()\n",
    "trainer.save_model(\"./out-cls-best\")\n",
    "tok.save_pretrained(\"./out-cls-best\")\n",
    "```\n",
    "\n",
    "**추론 스니펫**\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "tok = AutoTokenizer.from_pretrained(\"./out-cls-best\")\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(\"./out-cls-best\")\n",
    "clf = pipeline(\"text-classification\", model=mdl, tokenizer=tok)\n",
    "print(clf(\"이 제품 정말 마음에 들어요\"))  # [{'label': 'LABEL_1', 'score': ...}]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) 요약 — FLAN-T5 베이스로 가볍게\n",
    "\n",
    "* 모델: `google/flan-t5-base` (영/다국어 텍스트 요약에 실전성 좋음)\n",
    "\n",
    "```python\n",
    "# summarize_quick.py\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import os\n",
    "CACHE = r\"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "MODEL_ID = \"google/flan-t5-base\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE)\n",
    "mdl = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, cache_dir=CACHE)\n",
    "summarizer = pipeline(\"summarization\", model=mdl, tokenizer=tok)\n",
    "\n",
    "text = \"\"\"\n",
    "여기에 긴 문서 본문을 붙이세요. 여러 문단 가능.\n",
    "핵심만 깔끔히 뽑고 싶을 때 유용합니다.\n",
    "\"\"\"\n",
    "print(summarizer(text, max_length=160, min_length=60, do_sample=False, num_beams=4, no_repeat_ngram_size=3))\n",
    "```\n",
    "\n",
    "> 한국어 길면 `google/mt5-base`로 교체 가능. 도메인 적합화가 필요하면 LoRA로 문서-요약 페어 몇 천 샘플만으로도 미세튜닝 효과적.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) 대화/지시수행 — 소형 인스트럭트 LLM\n",
    "\n",
    "* 모델 예시: `Qwen/Qwen2-7B-Instruct`, `meta-llama/Meta-Llama-3-8B-Instruct`, `google/gemma-2-9b-it`\n",
    "\n",
    "```python\n",
    "# chat_quick.py\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "CACHE = r\"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "MODEL_ID = \"Qwen/Qwen2-7B-Instruct\"  # 원하시는 걸로 교체\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE, use_fast=True)\n",
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, cache_dir=CACHE, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "gen = pipeline(\"text-generation\", model=mdl, tokenizer=tok)\n",
    "\n",
    "system = \"당신은 간결하고 유용하게 답하는 한국어 비서입니다.\"\n",
    "user = \"RAG를 한 문장으로 설명해줘.\"\n",
    "prompt = f\"{system}\\n\\n사용자: {user}\\n어시스턴트:\"\n",
    "out = gen(prompt, max_new_tokens=200, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "print(out[0][\"generated_text\"])\n",
    "```\n",
    "\n",
    "> 저사양이면 GGUF(llama.cpp)나 4/8bit 양자화(AWQ/GPTQ/BNB)로 메모리 절약. 긴 컨텍스트가 필요하면 해당 크기/버전 옵션 확인.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) RAG — E5 임베딩 + FAISS + LLM 리더\n",
    "\n",
    "* **핵심**: 임베딩(검색) 품질이 절반 이상을 좌우. `intfloat/e5-base`로 간단히 시작.\n",
    "\n",
    "```python\n",
    "# rag_minimal.py\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss, numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "CACHE = r\"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "EMB = \"intfloat/e5-base\"  # 임베딩\n",
    "LLM = \"Qwen/Qwen2-7B-Instruct\"  # 생성기 (교체 가능)\n",
    "\n",
    "# 1) 문서 인덱싱\n",
    "docs = [\n",
    "    \"RAG는 검색으로 문맥을 보강한 후 생성하는 기법이다.\",\n",
    "    \"임베딩 품질이 RAG 성능에 큰 영향을 준다.\",\n",
    "    \"FAISS는 빠른 벡터 검색 라이브러리다.\",\n",
    "]\n",
    "embed = SentenceTransformer(EMB, cache_folder=CACHE)\n",
    "doc_vecs = embed.encode([f\"passage: {d}\" for d in docs], normalize_embeddings=True)\n",
    "index = faiss.IndexFlatIP(doc_vecs.shape[1]); index.add(doc_vecs)\n",
    "\n",
    "# 2) 쿼리 → 검색\n",
    "query = \"RAG가 뭐야?\"\n",
    "qv = embed.encode([f\"query: {query}\"], normalize_embeddings=True)\n",
    "D, I = index.search(qv, k=3)\n",
    "ctx = \"\\n\\n\".join([docs[i] for i in I[0]])\n",
    "\n",
    "# 3) LLM으로 답 생성(컨텍스트 주입)\n",
    "tok = AutoTokenizer.from_pretrained(LLM, cache_dir=CACHE, use_fast=True)\n",
    "mdl = AutoModelForCausalLM.from_pretrained(LLM, cache_dir=CACHE, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, device_map=\"auto\")\n",
    "gen = pipeline(\"text-generation\", model=mdl, tokenizer=tok)\n",
    "\n",
    "prompt = f\"\"\"다음 컨텍스트를 바탕으로 사용자의 질문에 답하세요.\n",
    "컨텍스트:\n",
    "{ctx}\n",
    "\n",
    "질문: {query}\n",
    "답변:\"\"\"\n",
    "resp = gen(prompt, max_new_tokens=200, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(resp[0][\"generated_text\"])\n",
    "```\n",
    "\n",
    "> 품질 업그레이드: 문서 chunking(예: 300\\~600토큰), top-k 3\\~5, MMR 다양성, **Cross-Encoder rerank** 추가, 컨텍스트 길이 확장 가능한 LLM 사용.\n",
    "\n",
    "---\n",
    "\n",
    "## 선택 가이드 (짧게)\n",
    "\n",
    "* **라벨 분류/태깅/스팸·감성** → (1) 분류 LoRA\n",
    "* **요약/정제/형식 변환** → (2) 요약 T5\n",
    "* **대화/작성/지시수행** → (3) 인스트럭트 LLM\n",
    "* **사내문서/웹지식+정확한 답** → (4) RAG\n",
    "\n",
    "원하시면 위 네 가지 중 **지금 바로 돌릴 태스크 하나**를 골라,\n",
    "\n",
    "* 데이터 예시(샘플 CSV 생성),\n",
    "* 학습/평가 커맨드,\n",
    "* 배포(로컬 API/vLLM/llama.cpp)\n",
    "  까지 한 번에 맞춰서 더 깔끔하게 드릴게요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac89e30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6b069d8",
   "metadata": {},
   "source": [
    "# DO LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2355fb32",
   "metadata": {},
   "source": [
    "좋아요. 제가 \\*\\*문서 분류(한국어 감성 분류 예시)\\*\\*를 골라서, 지금 바로 돌릴 수 있게 **데이터 생성 → LoRA 미세튜닝 → 평가 → 추론**까지 한 번에 준비해드릴게요.\n",
    "(전제: Windows/Conda, 캐시 `D:\\WorkSpace\\Hands_on_Gen_AI`)\n",
    "\n",
    "---\n",
    "\n",
    "## 0) 필수 패키지 + 캐시 경로 (한 번만)\n",
    "\n",
    "```powershell\n",
    "pip install -U transformers accelerate peft datasets scikit-learn\n",
    "setx HF_HOME \"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "setx TRANSFORMERS_CACHE \"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "```\n",
    "\n",
    "> 터미널/IDE 재시작 후 진행 권장\n",
    "\n",
    "---\n",
    "\n",
    "## 1) 샘플 데이터 만들기 (train/valid CSV 자동 생성)\n",
    "\n",
    "아래 스크립트가 **작은 한국어 감성 데이터셋**을 생성해 `train.csv`, `valid.csv`를 만듭니다.\n",
    "\n",
    "* 컬럼: `text,label` (0=부정, 1=긍정)\n",
    "\n",
    "```python\n",
    "# make_data.py\n",
    "import csv, os, random\n",
    "\n",
    "random.seed(42)\n",
    "train_data_pos = [\n",
    "    \"정말 만족스러워요\", \"제품이 기대 이상이에요\", \"친절하고 빠른 서비스네요\", \"다음에도 또 살게요\",\n",
    "    \"품질이 아주 좋아요\", \"설명이 이해하기 쉬워요\", \"디자인이 마음에 들어요\", \"가격 대비 최고입니다\"\n",
    "]\n",
    "train_data_neg = [\n",
    "    \"별로에요\", \"품질이 실망스럽네요\", \"설치가 너무 어려웠어요\", \"다시는 안 살래요\",\n",
    "    \"설명이 불친절해요\", \"가격이 너무 비싸요\", \"반품하고 싶어요\", \"추천하기 어렵네요\"\n",
    "]\n",
    "\n",
    "def make_split(pos, neg, ratio=0.75):\n",
    "    size_pos = int(len(pos) * ratio)\n",
    "    size_neg = int(len(neg) * ratio)\n",
    "    tr = [(t,1) for t in random.sample(pos, len(pos))[:size_pos]] + [(t,0) for t in random.sample(neg, len(neg))[:size_neg]]\n",
    "    va = [(t,1) for t in pos[size_pos:]] + [(t,0) for t in neg[size_neg:]]\n",
    "    random.shuffle(tr); random.shuffle(va)\n",
    "    return tr, va\n",
    "\n",
    "train, valid = make_split(train_data_pos, train_data_neg, ratio=0.75)\n",
    "\n",
    "def write_csv(path, rows):\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"text\",\"label\"])\n",
    "        for t,l in rows:\n",
    "            w.writerow([t, l])\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "write_csv(\"./data/train.csv\", train)\n",
    "write_csv(\"./data/valid.csv\", valid)\n",
    "print(\"Wrote ./data/train.csv and ./data/valid.csv\")\n",
    "```\n",
    "\n",
    "실행:\n",
    "\n",
    "```powershell\n",
    "python make_data.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) LoRA 미세튜닝 스크립트 (KLUE RoBERTa)\n",
    "\n",
    "* 모델: `klue/roberta-base` (한국어 태스크에 강함)\n",
    "* **LoRA**로 가볍게 튜닝 → VRAM/시간 절약\n",
    "\n",
    "```python\n",
    "# train_classification_lora.py\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "CACHE = r\"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "MODEL_ID = \"klue/roberta-base\"\n",
    "NUM_LABELS = 2\n",
    "MAX_LEN = 128\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE)\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID, num_labels=NUM_LABELS, cache_dir=CACHE\n",
    ")\n",
    "\n",
    "# LoRA 설정 (RoBERTa 계열 공통적으로 잘 먹는 모듈들)\n",
    "peft_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n",
    "    target_modules=[\"query\",\"key\",\"value\",\"dense\"]\n",
    ")\n",
    "model = get_peft_model(base, peft_cfg)\n",
    "\n",
    "# CSV 로드\n",
    "ds = load_dataset(\"csv\", data_files={\"train\":\"./data/train.csv\",\"valid\":\"./data/valid.csv\"})\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "\n",
    "ds = ds.map(tok_fn, batched=True)\n",
    "ds = ds.rename_column(\"label\", \"labels\")\n",
    "ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./out-cls\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=ds[\"train\"], eval_dataset=ds[\"valid\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./out-cls-best\")\n",
    "tok.save_pretrained(\"./out-cls-best\")\n",
    "print(\"Saved to ./out-cls-best\")\n",
    "```\n",
    "\n",
    "실행:\n",
    "\n",
    "```powershell\n",
    "python train_classification_lora.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) 평가/추론 (학습 결과 사용)\n",
    "\n",
    "```python\n",
    "# infer_classification.py\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"./out-cls-best\")\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(\"./out-cls-best\")\n",
    "\n",
    "clf = pipeline(\"text-classification\", model=mdl, tokenizer=tok, return_all_scores=False)\n",
    "samples = [\n",
    "    \"정말 만족스러워요\",\n",
    "    \"별로에요\",\n",
    "    \"가격은 비싸지만 품질은 좋아요\",\n",
    "]\n",
    "for s in samples:\n",
    "    print(s, \"->\", clf(s))\n",
    "```\n",
    "\n",
    "실행:\n",
    "\n",
    "```powershell\n",
    "python infer_classification.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4) 바로 적용 팁 (작업에 맞게 바꾸기)\n",
    "\n",
    "* **라벨 수 변경**: `NUM_LABELS`를 바꾸고 CSV의 `label` 범위를 맞추세요.\n",
    "* **데이터 늘리기**: `./data/train.csv`, `./data/valid.csv`에 행만 추가하면 됩니다(수천\\~수만 행까지).\n",
    "* **길이 조절**: 긴 문서는 `MAX_LEN`을 256\\~512로 늘리되, 배치/VRAM을 상황에 맞게 줄이세요.\n",
    "* **속도/메모리**: GPU 없으면 `fp16=False`(자동), 배치 축소. 더 줄이고 싶다면 4/8bit 로드시 bitsandbytes를 추가로 세팅할 수 있습니다.\n",
    "* **멀티 클래스**: `NUM_LABELS` 늘리고 CSV에 해당 라벨을 사용하세요.\n",
    "\n",
    "---\n",
    "\n",
    "원하시면 위 분류 파이프라인을 **당신의 실제 데이터 구조**(칼럼명/라벨 체계)로 맞춰 스크립트까지 즉시 변환해 드릴게요. 데이터 예시 몇 줄만 보내주시면 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3292e80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecd22c08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0287b103",
   "metadata": {},
   "source": [
    "# DO RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164e687",
   "metadata": {},
   "source": [
    "좋아요! \\*\\*RAG(검색 증강 생성)\\*\\*을 바로 돌릴 수 있게, **문서 준비 → 인덱싱 → 질의 → 생성(출처 표기)** 까지 “최소 노력” 세트로 드릴게요.\n",
    "(전제: Windows/Conda, 캐시는 `D:\\WorkSpace\\Hands_on_Gen_AI`, 로컬 LLM은 **Qwen2-7B-Instruct** 예시. GPU 없으면 더 작은 모델로 바꾸세요.)\n",
    "\n",
    "---\n",
    "\n",
    "## 0) 설치 & 캐시 경로 (한 번만)\n",
    "\n",
    "```powershell\n",
    "pip install -U transformers accelerate sentence-transformers faiss-cpu\n",
    "# 선택(재순위기 추가): pip install -U cross-encoder\n",
    "setx HF_HOME \"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "setx TRANSFORMERS_CACHE \"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1) 지식 문서 샘플 만들기\n",
    "\n",
    "작동 확인용 KB(지식베이스) 문서 몇 개를 만듭니다.\n",
    "\n",
    "```python\n",
    "# make_kb.py\n",
    "import os, textwrap\n",
    "os.makedirs(\"./kb\", exist_ok=True)\n",
    "\n",
    "docs = {\n",
    "    \"rag_intro.md\": \"\"\"\n",
    "RAG(Retrieval-Augmented Generation)은 외부 문서를 검색하여\n",
    "그 결과를 컨텍스트로 주입한 뒤 답변을 생성하는 기법이다.\n",
    "최신성·사실성·출처 제시가 필요한 QA에 유용하다.\n",
    "핵심 구성은 임베딩/벡터DB(FAISS 등), 리트리버, 리더(LLM)이다.\n",
    "\"\"\",\n",
    "    \"embeddings.md\": \"\"\"\n",
    "임베딩 모델(E5, BGE 등)은 텍스트를 고차원 벡터로 변환한다.\n",
    "좋은 임베딩일수록 의미적으로 가까운 텍스트들이 벡터 공간에서도 가깝다.\n",
    "RAG에서 임베딩 품질은 검색/정답률에 큰 영향을 준다.\n",
    "\"\"\",\n",
    "    \"faiss.md\": \"\"\"\n",
    "FAISS는 Facebook AI가 만든 고성능 벡터 검색 라이브러리이다.\n",
    "CPU 버전만으로도 수만~수백만 스케일에서 빠른 유사도 검색을 제공한다.\n",
    "지표로는 내적 또는 코사인 유사도를 많이 사용한다.\n",
    "\"\"\",\n",
    "    \"qwen2.md\": \"\"\"\n",
    "Qwen2-7B-Instruct는 오픈 LLM 중 하나로, 한국어를 포함한 다국어 지시응답에 강하다.\n",
    "RAG의 리더(생성기)로 사용하면 검색 컨텍스트를 바탕으로 간결한 답변을 생성할 수 있다.\n",
    "온도(temperature)를 낮추면 사실성 유지에 도움이 된다.\n",
    "\"\"\"\n",
    "}\n",
    "for fn, body in docs.items():\n",
    "    with open(os.path.join(\"./kb\", fn), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(textwrap.dedent(body).strip() + \"\\n\")\n",
    "\n",
    "print(\"KB written to ./kb/*.md\")\n",
    "```\n",
    "\n",
    "실행:\n",
    "\n",
    "```powershell\n",
    "python make_kb.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) 인덱스 빌드 (Chunk → Embedding → FAISS 저장)\n",
    "\n",
    "* **임베딩**: `intfloat/e5-base` (간단·성능/속도 밸런스 좋음, 다국어 OK)\n",
    "* **청킹**: 길이 300\\~600자 권장(예시는 400자)\n",
    "* **저장물**: `./index/faiss.index`(벡터), `./index/meta.npy`(문서/청크 메타)\n",
    "\n",
    "```python\n",
    "# build_index.py\n",
    "import os, glob, numpy as np, faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "CACHE = r\"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "EMB_ID = \"intfloat/e5-base\"  # 필요 시 'bge-m3' 등으로 교체\n",
    "CHUNK_SIZE = 400\n",
    "OVERLAP = 50\n",
    "\n",
    "def chunk_text(text, size=400, overlap=50):\n",
    "    out = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + size)\n",
    "        out.append(text[start:end])\n",
    "        start = end - overlap\n",
    "        if start < 0: start = 0\n",
    "    return [c.strip() for c in out if c.strip()]\n",
    "\n",
    "# 1) 문서 로드 & 청킹\n",
    "paths = sorted(glob.glob(\"./kb/*.md\"))\n",
    "docs = []\n",
    "for p in paths:\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        t = f.read()\n",
    "    chunks = chunk_text(t, CHUNK_SIZE, OVERLAP)\n",
    "    for i, ch in enumerate(chunks):\n",
    "        docs.append({\"doc_path\": p, \"chunk_id\": i, \"text\": ch})\n",
    "\n",
    "# 2) 임베딩\n",
    "embed = SentenceTransformer(EMB_ID, cache_folder=CACHE)\n",
    "texts = [f\"passage: {d['text']}\" for d in docs]\n",
    "vecs = embed.encode(texts, normalize_embeddings=True)\n",
    "\n",
    "# 3) FAISS 인덱스(IP = 코사인과 동치 when normalized)\n",
    "dim = vecs.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(vecs)\n",
    "\n",
    "# 4) 저장\n",
    "os.makedirs(\"./index\", exist_ok=True)\n",
    "faiss.write_index(index, \"./index/faiss.index\")\n",
    "np.save(\"./index/meta.npy\", np.array(docs, dtype=object))\n",
    "print(f\"Built {len(docs)} chunks, saved to ./index\")\n",
    "```\n",
    "\n",
    "실행:\n",
    "\n",
    "```powershell\n",
    "python build_index.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) 질의 → 검색 → 생성(출처 포함)\n",
    "\n",
    "* **리더(LLM)**: `Qwen/Qwen2-7B-Instruct` (오픈, 한국어 잘함)\n",
    "\n",
    "  * GPU 없으면 더 작은 `Qwen2-1.5B-Instruct` 또는 `gemma-2-2b-it`로 바꾸세요.\n",
    "* **검색**: top-k(기본 4) + 간단한 **MMR 다양성** 포함(선택)\n",
    "* **프롬프트**: 사실성 강화 위해 **temperature 낮음(0.2)**, **출처 요약**해서 함께 표기\n",
    "\n",
    "```python\n",
    "# rag_query.py\n",
    "import os, numpy as np, faiss, torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "CACHE = r\"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "EMB_ID = \"intfloat/e5-base\"\n",
    "LLM_ID = \"Qwen/Qwen2-7B-Instruct\"  # 더 작게: \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "TOP_K = 4\n",
    "USE_MMR = True\n",
    "MMR_LAMBDA = 0.7  # 1.0=유사도 위주, 0.0=다양성 위주\n",
    "\n",
    "# 1) 인덱스/메타 로드\n",
    "index = faiss.read_index(\"./index/faiss.index\")\n",
    "meta = np.load(\"./index/meta.npy\", allow_pickle=True)\n",
    "\n",
    "# 2) 임베딩 모델 로드\n",
    "embed = SentenceTransformer(EMB_ID, cache_folder=CACHE)\n",
    "\n",
    "def search(query, top_k=TOP_K, use_mmr=USE_MMR, mmr_lambda=MMR_LAMBDA):\n",
    "    qv = embed.encode([f\"query: {query}\"], normalize_embeddings=True)\n",
    "    D, I = index.search(qv, top_k*4 if use_mmr else top_k)\n",
    "    cand = I[0].tolist()\n",
    "    if not use_mmr:\n",
    "        return cand[:top_k]\n",
    "\n",
    "    # 간단 MMR\n",
    "    selected = []\n",
    "    cand_vecs = None\n",
    "    # 미리 후보 벡터 모으기\n",
    "    def get_vec(i):\n",
    "        nonlocal cand_vecs\n",
    "        if cand_vecs is None:\n",
    "            # 메모리를 아끼려고, 다시 임베딩 (원본 vecs 저장 안 했으니까)\n",
    "            texts = [f\"passage: {meta[j]['text']}\" for j in cand]\n",
    "            cand_vecs = embed.encode(texts, normalize_embeddings=True)\n",
    "        idx = cand.index(i)\n",
    "        return cand_vecs[idx]\n",
    "\n",
    "    q = qv[0]\n",
    "    while len(selected) < top_k and cand:\n",
    "        if not selected:\n",
    "            best = cand[0]\n",
    "            selected.append(best); cand.remove(best)\n",
    "        else:\n",
    "            best_i, best_score = None, -1e9\n",
    "            for i in cand:\n",
    "                sim_to_query = float(np.dot(get_vec(i), q))\n",
    "                sim_to_selected = max(float(np.dot(get_vec(i), get_vec(s))) for s in selected) if selected else 0.0\n",
    "                score = mmr_lambda * sim_to_query - (1 - mmr_lambda) * sim_to_selected\n",
    "                if score > best_score:\n",
    "                    best_score, best_i = score, i\n",
    "            selected.append(best_i); cand.remove(best_i)\n",
    "    return selected\n",
    "\n",
    "# 3) LLM 로드\n",
    "tok = AutoTokenizer.from_pretrained(LLM_ID, cache_dir=CACHE, use_fast=True)\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "mdl = AutoModelForCausalLM.from_pretrained(LLM_ID, cache_dir=CACHE, torch_dtype=dtype, device_map=\"auto\")\n",
    "gen = pipeline(\"text-generation\", model=mdl, tokenizer=tok)\n",
    "\n",
    "def answer(query):\n",
    "    hits = search(query)\n",
    "    contexts = [meta[i] for i in hits]\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(\n",
    "        [f\"[{k+1}] {os.path.basename(c['doc_path'])} (chunk {c['chunk_id']}):\\n{c['text']}\" for k, c in enumerate(contexts)]\n",
    "    )\n",
    "    prompt = f\"\"\"당신은 신뢰할 수 있는 한국어 어시스턴트입니다.\n",
    "다음 '컨텍스트'만을 근거로 간결하고 정확하게 답하세요.\n",
    "가능하면 답변 끝에 [1], [2]처럼 참조 번호를 포함해 출처를 표시하세요.\n",
    "모르겠으면 모른다고 답하세요.\n",
    "\n",
    "컨텍스트:\n",
    "{context_text}\n",
    "\n",
    "질문: {query}\n",
    "답변:\"\"\"\n",
    "    out = gen(\n",
    "        prompt, max_new_tokens=300, temperature=0.2, top_p=0.9, do_sample=True,\n",
    "        repetition_penalty=1.05, pad_token_id=tok.eos_token_id\n",
    "    )[0][\"generated_text\"]\n",
    "    # prompt를 포함한 전체를 내보내니, 답만 추출(간단 파싱)\n",
    "    ans = out.split(\"답변:\")[-1].strip()\n",
    "    return ans, contexts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"RAG의 핵심 구성 요소는 무엇이고 왜 임베딩이 중요한가?\"\n",
    "    ans, ctxs = answer(q)\n",
    "    print(\"\\nQ:\", q)\n",
    "    print(\"\\nA:\", ans)\n",
    "    print(\"\\n[참고 출처]\")\n",
    "    for i, c in enumerate(ctxs, 1):\n",
    "        print(f\"[{i}] {os.path.basename(c['doc_path'])}#chunk{c['chunk_id']}\")\n",
    "```\n",
    "\n",
    "실행:\n",
    "\n",
    "```powershell\n",
    "python rag_query.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4) (선택) 재순위기(Reranker)로 품질 끌어올리기\n",
    "\n",
    "검색 상위 8\\~10개를 뽑은 뒤 **Cross-Encoder**로 상위 3\\~5개만 LLM에 전달하면 답변 품질이 더 좋아집니다.\n",
    "\n",
    "```python\n",
    "# add to rag_query.py (옵션)\n",
    "USE_RERANK = False\n",
    "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # pip install cross-encoder\n",
    "\n",
    "if USE_RERANK:\n",
    "    from cross_encoder import CrossEncoder\n",
    "    reranker = CrossEncoder(RERANK_MODEL)\n",
    "\n",
    "def rerank(query, indices):\n",
    "    if not USE_RERANK: return indices\n",
    "    pairs = [(query, meta[i][\"text\"]) for i in indices]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked = [x for _, x in sorted(zip(scores, indices), reverse=True)]\n",
    "    return ranked\n",
    "```\n",
    "\n",
    "> `search()`의 반환 직후 `indices = rerank(query, indices)` 형태로 적용.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) 운영 팁 (짧게)\n",
    "\n",
    "* **청킹**: 300\\~600자(또는 200\\~400토큰) + 10\\~20% 중첩 → 회수/정확 균형\n",
    "* **임베딩 프롬프트**: E5 류는 `\"query: ...\"`, `\"passage: ...\"` 프리픽스가 성능에 중요\n",
    "* **하이퍼파라미터**: `TOP_K=3~5`, `temperature=0.2~0.5`, MMR로 중복 줄이기\n",
    "* **출처 표기**: 문서 파일명+chunk 번호(or URL) 함께 노출\n",
    "* **확장**: 문서 수↑ → **FAISS IVF/HNSW** 등 근사탐색로 전환, 다국어면 다국어 임베딩 사용\n",
    "* **긴 문맥**: 긴 컨텍스트 지원 LLM(예: Qwen2 최신, Llama 계열) + FlashAttention2 활성 옵션\n",
    "\n",
    "---\n",
    "\n",
    "## 6) 한 줄로 요약\n",
    "\n",
    "* 이 스크립트 세트(`make_kb.py` → `build_index.py` → `rag_query.py`)만 실행하면 **로컬 RAG**가 즉시 동작하고, 답변에 **출처**가 붙습니다.\n",
    "* 다음 단계로는 **재순위기**, **긴 컨텍스트 LLM**, **파일 업로드/크롤링 파이프라인**을 차근히 붙이면 됩니다.\n",
    "\n",
    "원하시면 이걸 **API(FastAPI) 형태**로 바꿔서 `/ask?q=...`로 호출 가능하게도 바로 만들어 드릴게요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d2b50",
   "metadata": {},
   "source": [
    "좋아요—**진짜 최소한**으로 “어떻게 붙이는지 감만 오게”만 적습니다. 필요해지면 키워드로 찾아 확장하면 돼요.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) FastAPI로 `/ask?q=...` 만들기 (RAG 래핑)\n",
    "\n",
    "**핵심 흐름**: (로드) 인덱스/임베딩/LLM → (요청) 쿼리 받기 → 검색 →(옵션: 재순위)→ 컨텍스트 붙여 LLM 생성 → JSON 반환\n",
    "\n",
    "```python\n",
    "# app.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import faiss, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch, os\n",
    "\n",
    "CACHE = r\"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "EMB_ID = \"intfloat/e5-base\"\n",
    "LLM_ID = \"Qwen/Qwen2-7B-Instruct\"  # 3080이면 4bit or 더 작은 모델 권장\n",
    "TOP_K = 4\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# --- 로드 (앱 시작 시 1회)\n",
    "index = faiss.read_index(\"./index/faiss.index\")\n",
    "meta  = np.load(\"./index/meta.npy\", allow_pickle=True)\n",
    "embed = SentenceTransformer(EMB_ID, cache_folder=CACHE)\n",
    "tok   = AutoTokenizer.from_pretrained(LLM_ID, cache_dir=CACHE, use_fast=True)\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "mdl   = AutoModelForCausalLM.from_pretrained(LLM_ID, cache_dir=CACHE, torch_dtype=dtype, device_map=\"auto\")\n",
    "gen   = pipeline(\"text-generation\", model=mdl, tokenizer=tok)\n",
    "\n",
    "def retrieve(query, k=TOP_K):\n",
    "    qv = embed.encode([f\"query: {query}\"], normalize_embeddings=True)\n",
    "    D, I = index.search(qv, k)\n",
    "    ctxs = [meta[i] for i in I[0]]\n",
    "    ctx  = \"\\n\\n---\\n\\n\".join([f\"[{j+1}] {os.path.basename(c['doc_path'])} (chunk {c['chunk_id']}):\\n{c['text']}\" for j,c in enumerate(ctxs)])\n",
    "    return ctx, ctxs\n",
    "\n",
    "class AskIn(BaseModel):\n",
    "    q: str\n",
    "\n",
    "@app.get(\"/ask\")\n",
    "def ask_get(q: str):\n",
    "    ctx, ctxs = retrieve(q)\n",
    "    prompt = f\"컨텍스트만 근거로 간결히 답하세요.\\n\\n컨텍스트:\\n{ctx}\\n\\n질문: {q}\\n답변:\"\n",
    "    out = gen(prompt, max_new_tokens=300, temperature=0.2, top_p=0.9, do_sample=True)[0][\"generated_text\"]\n",
    "    ans = out.split(\"답변:\")[-1].strip()\n",
    "    sources = [{\"file\": os.path.basename(c[\"doc_path\"]), \"chunk\": int(c[\"chunk_id\"])} for c in ctxs]\n",
    "    return {\"answer\": ans, \"sources\": sources}\n",
    "```\n",
    "\n",
    "실행:\n",
    "\n",
    "```powershell\n",
    "uvicorn app:app --reload --port 8000\n",
    "# 테스트: http://localhost:8000/ask?q=RAG가 뭔가요?\n",
    "```\n",
    "\n",
    "> 프론트에서 쓰려면 CORS 추가: `from fastapi.middleware.cors import CORSMiddleware` 로 `allow_origins=[\"*\"]` 등 설정.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) 재순위기(Reranker) 간단 붙이기\n",
    "\n",
    "**아이디어**: 검색 상위 n(예: 8) → Cross-Encoder로 점수 재계산 → 상위 k(예: 3\\~5)만 LLM에 전달.\n",
    "\n",
    "```python\n",
    "# 설치: pip install -U cross-encoder\n",
    "from cross_encoder import CrossEncoder\n",
    "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker = CrossEncoder(RERANK_MODEL)\n",
    "\n",
    "def rerank(query, indices):\n",
    "    pairs = [(query, meta[i][\"text\"]) for i in indices]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked = [x for _, x in sorted(zip(scores, indices), reverse=True)]\n",
    "    return ranked\n",
    "\n",
    "# 검색 함수에서: k를 좀 크게 뽑은 뒤 rerank → 상위 TOP_K 사용\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) 긴 컨텍스트 LLM으로 교체\n",
    "\n",
    "* **방법**: LLM\\_ID만 긴 컨텍스트 지원 모델로 바꾸고(예: Llama 3.1 8B/70B, Qwen2-7B-long 등), `max_new_tokens`/입력 길이 한도만 조절.\n",
    "* **3080 대안**: **양자화(4bit)** 또는 **소형 롱컨텍스트 모델**.\n",
    "* **프롬프트**: 긴 컨텍스트일수록 **온도 낮게(0.2\\~0.5)**, **repetition\\_penalty** 소폭↑.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) 파일 업로드 파이프라인 (간단)\n",
    "\n",
    "**흐름**: `/upload`에서 파일 수신 → 텍스트 추출 → 청킹 → 임베딩 → **기존 인덱스에 add** 또는 재빌드\n",
    "\n",
    "```python\n",
    "# pip install python-multipart pypdf\n",
    "from fastapi import UploadFile, File\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(fp) -> str:\n",
    "    r = PdfReader(fp); return \"\\n\".join(p.extract_text() or \"\" for p in r.pages)\n",
    "\n",
    "@app.post(\"/upload\")\n",
    "async def upload(file: UploadFile = File(...)):\n",
    "    if file.filename.lower().endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file.file)\n",
    "    else:\n",
    "        text = (await file.read()).decode(\"utf-8\", errors=\"ignore\")\n",
    "    # -> chunk_text(), 임베딩 encode, faiss_index.add(vecs), meta append & 저장\n",
    "    return {\"ok\": True, \"filename\": file.filename}\n",
    "```\n",
    "\n",
    "> 운영에선 비동기 작업큐(RQ/Celery)로 인덱스 업데이트를 백그라운드 처리하는 게 안정적.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) 간단 크롤링 파이프라인\n",
    "\n",
    "* **옵션 A(간단)**: `trafilatura`로 HTML → 텍스트 추출\n",
    "* **옵션 B(뉴스/블로그)**: `newspaper3k`\n",
    "* **옵션 C(사내 위키/API)**: 해당 API로 수집\n",
    "\n",
    "```python\n",
    "# pip install trafilatura\n",
    "import trafilatura, requests\n",
    "\n",
    "def fetch_text(url: str) -> str:\n",
    "    html = requests.get(url, timeout=15).text\n",
    "    return trafilatura.extract(html) or \"\"\n",
    "\n",
    "# 사용: text = fetch_text(\"https://example.com/page\")\n",
    "# -> 파일 업로드와 동일하게 chunk → embed → index에 추가\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6) 키워드만 더\n",
    "\n",
    "* **FAISS 대규모**: IVF/HNSW, OPQ(양자화) 검색 속도↑ 메모리↓\n",
    "* **MMR 다양성**: 중복 줄여 정보량↑\n",
    "* **프롬프트 가드레일**: “컨텍스트에 없는 것은 모른다고 답해”\n",
    "* **로깅/관측**: 쿼리, 히트 문서, 답변, 소요시간 기록 → 품질 개선 루프\n",
    "* **배포**: `uvicorn` 단독 → `gunicorn -k uvicorn.workers.UvicornWorker` + 리버스 프록시(Nginx)\n",
    "\n",
    "---\n",
    "\n",
    "이 정도 틀만 잡아도 **/ask**, **업로드**, **크롤링**까지 스몰-스텝으로 붙일 수 있어요.\n",
    "필요해지면 *한 부분씩* 파고들면 됩니다: “FastAPI CORS”, “Cross-Encoder rerank”, “FAISS IVF”, “trafilatura usage”, “bitsandbytes 4bit” 같은 키워드로요.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
