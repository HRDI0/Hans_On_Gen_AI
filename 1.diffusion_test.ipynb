{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42b6c2a",
   "metadata": {},
   "source": [
    "간단히 말해:\n",
    "\n",
    "* **양자화(quantization)**: 모델의 가중치/연산 정밀도를 낮춰서(**int8/float8/4bit 등**) **VRAM을 줄이고** **속도를 올리는** 기법이에요. 보통 약간의 품질 손실과 맞바꿉니다. ([Hugging Face][1])\n",
    "* **Quanto(옵티멈-Quanto)**: Hugging Face가 제공하는 **파이토치 양자화 백엔드**로, Diffusers/Transformers에서 손쉽게 양자화를 쓰게 해줍니다. **float8/int8/int4/int2** 가중치 양자화를 지원하고 `torch.compile`과도 궁합이 좋아요. ([Hugging Face][2])\n",
    "\n",
    "따라서 “(선택) 양자화/quanto를 쓸 계획이라면”은 **VRAM이 빡빡하거나(예: 8–12GB), 큰 모델(Flux/SDXL 등)을 쓰고 싶거나, 추론을 더 빠르게 돌리고 싶다면** `optimum-quanto`를 추가로 설치하라는 뜻이에요. 기본 사용만 한다면 꼭 설치할 필요는 없습니다. ([Hugging Face][3])\n",
    "\n",
    "### 어떻게 쓰나? (예시)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from diffusers import FluxTransformer2DModel, FluxPipeline, QuantoConfig\n",
    "\n",
    "model_id = \"black-forest-labs/FLUX.1-dev\"\n",
    "qconf = QuantoConfig(weights_dtype=\"float8\")  # int8 / int4 / int2 도 가능\n",
    "\n",
    "# 트랜스포머 부분만 양자화해서 로드\n",
    "transformer = FluxTransformer2DModel.from_pretrained(\n",
    "    model_id, subfolder=\"transformer\",\n",
    "    quantization_config=qconf, torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "img = pipe(\"hello world sign cat\", num_inference_steps=30).images[0]\n",
    "img.save(\"out.png\")\n",
    "```\n",
    "\n",
    "(설치: `pip install optimum-quanto accelerate`) ([Hugging Face][4])\n",
    "\n",
    "원하시면 **GPU VRAM 용량**이랑 **돌리고 싶은 모델** 알려주세요. “양자화 안 함 / float8 / int8 / int4” 중 어떤 게 맞을지 바로 추천해 드릴게요.\n",
    "\n",
    "[1]: https://huggingface.co/docs/diffusers/en/api/quantization?utm_source=chatgpt.com \"Quantization\"\n",
    "[2]: https://huggingface.co/docs/transformers/main/en/quantization/quanto?utm_source=chatgpt.com \"Optimum Quanto\"\n",
    "[3]: https://huggingface.co/blog/quanto-diffusers?utm_source=chatgpt.com \"Memory-efficient Diffusion Transformers with Quanto and ...\"\n",
    "[4]: https://huggingface.co/docs/diffusers/en/quantization/quanto \"Quanto\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa45eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import os\n",
    "# os.environ[\"XFORMERS_DISABLE_FLASH3\"] = \"1\"\n",
    "from diffusers import (\n",
    "    StableDiffusionXLPipeline,\n",
    "    StableDiffusionXLImg2ImgPipeline,\n",
    ")\n",
    "\n",
    "# -------------------\n",
    "# 환경 기본 세팅 (3080 최적)\n",
    "# -------------------\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")  # PyTorch 2.0+\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "BASE_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "REFINER_ID = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
    "# cache_path = r\"D:\\WorkSpace\\Hands_on_Gen_AI\"\n",
    "cache_path = r\"./\"\n",
    "\n",
    "# -------------------\n",
    "# 파이프라인 로드\n",
    "# -------------------\n",
    "pipe_base = StableDiffusionXLPipeline.from_pretrained(\n",
    "    BASE_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    "    cache_dir=cache_path,\n",
    ")\n",
    "pipe_refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    REFINER_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    "    cache_dir=cache_path,\n",
    ")\n",
    "\n",
    "# 속도/메모리 옵션 (xformers 우선)\n",
    "for p in (pipe_base, pipe_refiner):\n",
    "    try:\n",
    "        p.enable_xformers_memory_efficient_attention()\n",
    "        print(\"xFormers enabled\")\n",
    "    except Exception:\n",
    "        p.enable_attention_slicing()\n",
    "        print(\"xFormers not available -> attention_slicing enabled\")\n",
    "\n",
    "    # 1024 이상에서 안정성↑\n",
    "    p.enable_vae_tiling()\n",
    "\n",
    "pipe_base = pipe_base.to(DEVICE)\n",
    "pipe_refiner = pipe_refiner.to(DEVICE)\n",
    "\n",
    "# 진행바 표시(내장 tqdm)\n",
    "pipe_base.set_progress_bar_config(desc=\"SDXL Base\", leave=True)\n",
    "pipe_refiner.set_progress_bar_config(desc=\"SDXL Refiner\", leave=True)\n",
    "\n",
    "# -------------------\n",
    "# 생성 함수 (OOM 자동 대처 포함)\n",
    "# -------------------\n",
    "def generate_sdxl_3080(\n",
    "    prompt: str,\n",
    "    negative_prompt: str = \"text, watermark, signature, blurry, low quality, deformed, extra fingers\",\n",
    "    steps: int = 50,            # 40~50 권장\n",
    "    guidance: float = 6.0,      # SDXL는 5~7 구간 추천\n",
    "    seed: int = 42,\n",
    "    out_path: str = \"sdxl_base_refiner_3080.png\",\n",
    "    start_res=(1024, 1024),     # 기본 1024\n",
    "    fallback_res=((896, 896), (768, 768)),  # OOM 시 자동 다운스케일\n",
    "    denoise_split: float = 0.8, # Base 80% + Refiner 20%\n",
    "):\n",
    "    g = torch.Generator(device=DEVICE).manual_seed(seed)\n",
    "\n",
    "    tried = []\n",
    "    for (H, W) in (start_res,) + tuple(fallback_res):\n",
    "        try:\n",
    "            print(f\"\\n[Try] {H}x{W}, steps={steps}, guidance={guidance} (seed={seed})\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # 1단계: Base (latent로 출력, denoising_end=0.8)\n",
    "            with torch.inference_mode(), torch.autocast(device_type=DEVICE, dtype=torch.float16):\n",
    "                base_out = pipe_base(\n",
    "                    prompt=prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    height=H,\n",
    "                    width=W,\n",
    "                    num_inference_steps=steps,\n",
    "                    guidance_scale=guidance,\n",
    "                    denoising_end=denoise_split,\n",
    "                    output_type=\"latent\",   # Refiner에 latents 전달\n",
    "                    generator=g,\n",
    "                )\n",
    "            latents = base_out.images  # latent tensor\n",
    "\n",
    "            # 2단계: Refiner (denoising_start=0.8)\n",
    "            with torch.inference_mode(), torch.autocast(device_type=DEVICE, dtype=torch.float16):\n",
    "                refined = pipe_refiner(\n",
    "                    prompt=prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    image=latents,\n",
    "                    num_inference_steps=steps,\n",
    "                    guidance_scale=guidance,\n",
    "                    denoising_start=denoise_split,\n",
    "                    generator=g,\n",
    "                )\n",
    "\n",
    "            img = refined.images[0]\n",
    "            img.save(out_path)\n",
    "            print(f\"✅ Done: {out_path}  ({H}x{W})\")\n",
    "            return\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            tried.append((H, W))\n",
    "            msg = str(e)\n",
    "            if \"CUDA out of memory\" in msg or \"CUDNN_STATUS_NOT_SUPPORTED\" in msg:\n",
    "                print(f\"⚠️ OOM/CuDNN at {H}x{W}. Trying lower resolution...\")\n",
    "                continue\n",
    "            # 다른 런타임 에러면 즉시 raise\n",
    "            raise\n",
    "\n",
    "    # 여기까지 왔다는 건 모든 해상도에서 실패\n",
    "    raise RuntimeError(f\"모든 해상도 시도가 실패했습니다. 시도한 해상도: {tried}\")\n",
    "\n",
    "# -------------------\n",
    "# 사용 예시\n",
    "# -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = (\n",
    "        \"a photo of a beautiful girl riding a dragon on the rings of Saturn, \"\n",
    "        \"ultra-detailed, 4k, cinematic lighting, high detail, sharp focus\"\n",
    "    )\n",
    "    generate_sdxl_3080(\n",
    "        prompt=prompt,\n",
    "        steps=48,             # 3080에서 48 스텝이면 품질/속도 균형 좋아요\n",
    "        guidance=6.0,         # 과하면 경직됨 → 5.5~6.5 추천\n",
    "        seed=123,\n",
    "        out_path=\"sdxl_3080_base_refiner.png\",\n",
    "        start_res=(1024, 1024),\n",
    "        fallback_res=((896, 896), (768, 768)),\n",
    "        denoise_split=0.8,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ea6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\HSM\\.cache\\huggingface\\hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import StableDiffusionPipeline\n",
    "# import torch\n",
    "\n",
    "# model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# # 1) 토큰이 필요한 모델이면, 먼저 HF 로그인:\n",
    "# #    huggingface-cli login  (터미널에서 1회)\n",
    "# #    또는 아래처럼 use_auth_token=\"hf_xxx\" 인자 전달\n",
    "\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.float16,   # FP16로 VRAM 절약 + 속도\n",
    "#     # use_auth_token=\"hf_xxx\",   # 필요할 때만\n",
    "# )\n",
    "\n",
    "# # 2) 메모리 최적화 옵션(3080 10GB면 여유 있지만, 안정성↑)\n",
    "# pipe.enable_attention_slicing()   # attention chunking\n",
    "# pipe.enable_vae_tiling()          # 큰 해상도에서 OOM 예방\n",
    "# # (메모리 더 빡빡하면)\n",
    "# # pipe.enable_model_cpu_offload()  # 대신 .to(\"cuda\")는 생략\n",
    "# pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# # 3) 빠른/안정 추론 설정\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# prompt = \"a photo of an beautifull girl riding a dragon on Rings of Saturn\"\n",
    "\n",
    "# # 4) 단일 이미지 생성 + 저장\n",
    "# with torch.inference_mode(), torch.autocast(\"cuda\"):\n",
    "#     result = pipe(prompt, num_inference_steps=40, guidance_scale=7.0)\n",
    "# img = result.images[0]\n",
    "# img.save(\"astronaut_rides_horse.png\")\n",
    "\n",
    "# # 5) 여러 장 생성하고 저장 (예: 4장)\n",
    "# with torch.inference_mode(), torch.autocast(\"cuda\"):\n",
    "#     result = pipe(\n",
    "#         [prompt], \n",
    "#         num_images_per_prompt=4, \n",
    "#         num_inference_steps=40, \n",
    "#         guidance_scale=7.0\n",
    "#     )\n",
    "# for i, im in enumerate(result.images):\n",
    "#     im.save(f\"astronaut_rides_horse_{i}.png\")\n",
    "\n",
    "# print(\"Done. Saved 1 + 4 images.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxl_env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
